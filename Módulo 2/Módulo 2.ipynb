{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Módulo 2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nVUu3tGqlabJ",
        "LrQOAPOi7yOZ",
        "u7RjWZTaZP_y",
        "KCLVcshx9Lfy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHzsqT432vKR"
      },
      "source": [
        "# Capítulo 1: Introdução ao Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVUu3tGqlabJ"
      },
      "source": [
        "## 1.1 - O que é Spark?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAsmkNxkldtK"
      },
      "source": [
        "### Objetivos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL5DOuuimbqM"
      },
      "source": [
        "- conceitos fundamentais do Apache Spark\n",
        "- identificar oportunidades para construir aplicações usando o Spark\n",
        "- construir aplicações em Apache Spark, por meio de seus principais componentes, para processamento de dados em altíssima escala\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LJwRrCapnuu"
      },
      "source": [
        "### Desafios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOfgwlFNppsy"
      },
      "source": [
        "- volume de dados vem crescendo em taxa superior à capacidade de processamento\n",
        "- restrição física limita o aumento do poder computacional dos computadores\n",
        "\n",
        "- dados não cabem na memória de um único computador\n",
        "- desejo de interpretação dos dados e reação a eles em tempo real\n",
        "- sistemas tradicionais de banco de dados relacionais não escalam suficientemente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Z4yq2RzlWI"
      },
      "source": [
        "### Solução: Computação Distribuída"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gNYFY7Czp0U"
      },
      "source": [
        "- divisão em conjuntos de dados menores\n",
        "- processamento de dados de forma distribuída em múltiplas máquinas\n",
        "- grupo de computadores trabalham em conjunto como se fosse um único computador para o usuário\n",
        "- vasto campo de estudo na ciência da computação\n",
        "- sistemas de computação distribuída são complexos de se implementar\n",
        "    - cada máquina pode falhar\n",
        "    - necessidade de coordenar as máquinas entre si\n",
        "    - difíceis de implantar, manter, depurar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7itwx340A-V"
      },
      "source": [
        "### Plataformas de Computação Distribuída"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_uckN8K2EYs"
      },
      "source": [
        "- Hadoop\n",
        "- Flink\n",
        "- Apache Storm\n",
        "- Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Xef8ig2M67"
      },
      "source": [
        "### O que é o Apache Spark?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHu074Sk7dhC"
      },
      "source": [
        "- plataforma de código aberto\n",
        "- para processamento unificado de cargas de trabalho de big data\n",
        "- de forma rápida e distribuída"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyNEeFV42kLG"
      },
      "source": [
        "- big data\n",
        "- large-scale data\n",
        "- big data workloads\n",
        "<br>\n",
        "\n",
        "- distributed data processing\n",
        "- cluster computing\n",
        "<br>\n",
        "\n",
        "- general-purpose\n",
        "- unified \n",
        "<br>\n",
        "\n",
        "- lightning-fast\n",
        "<br>\n",
        "\n",
        "- open-souce: https://github.com/apache/spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVUxrMoQ3BiF"
      },
      "source": [
        "#### Ferramenta para Computação Distribuída"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Nok7F-3e8z"
      },
      "source": [
        "- paralelismo implícito\n",
        "    - o programador não precisa de preocupar em dividir as tarefas, nem em coordenar a computaçãoo entre as máquinas\n",
        "    - aumento da produtividade do programador\n",
        "\n",
        "- tolerância a falhas\n",
        "    - capacidade do sistema operar mesmo após falhas\n",
        "    - redundância e replicação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP1ZMijC4leg"
      },
      "source": [
        "#### Ferramenta Unificada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKUBWMTR4qCD"
      },
      "source": [
        "- processamento em lote (batch), ETL\n",
        "- processamento de streams (fluxo)\n",
        "- ML, Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEvBp3TO5BrC"
      },
      "source": [
        "#### É um Ecossistema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urOHuV8M5EsS"
      },
      "source": [
        "- Streaming\n",
        "- MLib (for ML)\n",
        "- GraphX (for graph computing)\n",
        "- Spark SQL & DataFrames (dados estruturados)\n",
        "- Spark Core API\n",
        "    - R\n",
        "    - Python\n",
        "    - Scala\n",
        "    - SQL\n",
        "    - Java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fy0yqjs5gmH"
      },
      "source": [
        "#### História do Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydvw8yC6UwZ"
      },
      "source": [
        "- nasceu na Universidade da Califórnia, Berkeley, em 2019\n",
        "- em 2013, foi doado à Fundação Apache\n",
        "- em 2013, os criadores do Spark fundaram a Databricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrQOAPOi7yOZ"
      },
      "source": [
        "## 1.2 - Quais as Vantagens e Desvantagens do Spark?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWGre3-g74Gm"
      },
      "source": [
        "### Por que aprender Apache Spark?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sS4jsFAmqe4"
      },
      "source": [
        "#### Desempenho\n",
        "\n",
        "- servidores baratos, com muita memória e várias cores\n",
        "- pouco I/O\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v29TjOFrVDpt"
      },
      "source": [
        "#### API Simples\n",
        "\n",
        "- estrutura de dados: RDD, Dataset, Dataframe\n",
        "- Operações:  transformações e ações"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DeVIRqEVo73"
      },
      "source": [
        "#### Suporte a várias linguagens\n",
        "\n",
        "- Python, Java, R, Scala\n",
        "- bindings para C#, F#, Julia, Groovy, Closure, Kotlin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N0hlzsYmy4E"
      },
      "source": [
        "#### Plataforma unificada para analytics\n",
        "\n",
        "- componentes para diferentes cargas de trabalho\n",
        "    - dados estruturados: Spark SQL (capítulo 4)\n",
        "    - dados em grafos: Spark Graph X (capítulo 5)\n",
        "    - dados em streaming: Spark Streaming (capítulo 6)\n",
        "    - dados para aprendizado de máquina: Spark MLLib (capítulo 7)\n",
        "\n",
        "- substitui:\n",
        "    - Apache Storm\n",
        "    - Google Dremel\n",
        "    - Google Pregel\n",
        "    - Apache Impala\n",
        "    - Haddop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTW9kJzyYPi6"
      },
      "source": [
        "### Quando evitar o Spark?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plD-uWHlY0X-"
      },
      "source": [
        "- o valume de dados não for grande\n",
        "    - o Spark é útil a a partir de dezenas/centenas de GBs\n",
        "- poucos recursos computacionais e pouca memória\n",
        "    - Spark é desenhado para operar com abundância de CPU e memória\n",
        "    - neste cenário, o Hadoop pode fazer mais sentido\n",
        "- busca de dados textuais\n",
        "    - Solr e elasticsearch podem fazer mais sentido"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7RjWZTaZP_y"
      },
      "source": [
        "## 1.3 - Estudo de Casos Reais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fz69AyBZyAA"
      },
      "source": [
        "### Quem usa o Apache Spark?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1N6vtvZaR6v"
      },
      "source": [
        "- cientistas de dados\n",
        "- engenheiros de dados\n",
        "- engenheiros de aprendizado de máquina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnq8ngMAac3c"
      },
      "source": [
        "#### Cientistas de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsn8EPWMap6S"
      },
      "source": [
        "- combinação de Matemática, Estatística, Ciência da Computação / Programação\n",
        "- contar estórias por meio dos dados\n",
        "- limpar os dados\n",
        "- descobrir padrões\n",
        "- construir modelos para previsão e recomendação\n",
        "\n",
        "##### Por que o Spark é útil?\n",
        "- Spark SQL (exploração interativa dos dados) - capítulo 4\n",
        "- Spark MLLib - capítulo 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndixhgcJcB_i"
      },
      "source": [
        "#### Engenheiros de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHlVM3lYcB7k"
      },
      "source": [
        "- implantar os modelos em produção\n",
        "- construir e transformar dados reais: sujos e crus\n",
        "- conectar com o pipeline de dados: aplicações web, Apache Kafka\n",
        "- transformar end-to-end os dados: ler de várias fontes e armazenar os dados na nuvem, em bancos SQL e NoSQL\n",
        "\n",
        "##### Por que o Spark é útil?\n",
        "- forma simples de paralelizar a computação\n",
        "- esconde a complexidade de distribuir os dados e capacidade de tolerar falhas\n",
        "- provê APIs para ler e combinar dados de múltiplas fontes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FITLKeLhdwQl"
      },
      "source": [
        "### Casos de uso típicos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi1E0k_dd3Ex"
      },
      "source": [
        "- processar em paralelo grandes conjuntos de dadoss\n",
        "- explorar e visualizar conjuntos de dados de forma interativa\n",
        "- construir, treinar e avaliar modelos de ML\n",
        "- implementar pipelines de dados end-to-end\n",
        "- analisar redes sociais e grafos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ZEBurXehF0"
      },
      "source": [
        "### Estudo de Caso #1 - Worldsense\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exgatPuKekQ9"
      },
      "source": [
        "- Common Crawl\n",
        "<br>\n",
        "\n",
        "- ~ 10 bilhões de links\n",
        "- 40 TB de dados\n",
        "- A WorldSense processava os dados em algumas horas em máquinas da AWS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD_T3j5wg_mJ"
      },
      "source": [
        "### Estudo de Caso  #2 - airbnb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYiDSOWchGr5"
      },
      "source": [
        "- pipeline de dados complexo\n",
        "- várias fontes e destinoss de dados distintas\n",
        "- Spark em batch e stream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVELvae8hOTX"
      },
      "source": [
        "### Estudo de Caso #3 - Intel "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBPh9PXthmvx"
      },
      "source": [
        "- Bioinformática\n",
        "<br\n",
        "\n",
        "- benefícios do Spark:\n",
        "    - Spark Core: não há necessidade de se preocupar em como paralelizar a computação => foco nos algoritmos\n",
        "    - GraphX: vários algoritmos que operam sobre grafos implementados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQKOtSGz7WLn"
      },
      "source": [
        "### Estudo de Caso #4 - NBC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSHJQ3qU7aVS"
      },
      "source": [
        "- NBC: uma das maiores empresas de mídia do mundo\n",
        "- time de infraestrutura precisa servir vídeos sob demanda\n",
        "<br><br>\n",
        "\n",
        "- cachear todo o conteúdo: baixa latência, alto custo\n",
        "- não cachear nada: alta latência para os consumidores\n",
        "- problema de negócio: como encontrar o menor compromiso?\n",
        "<br><br>\n",
        "\n",
        "- GBs de metadados sobre os vídeos:\n",
        "    - IDs dos vídeos\n",
        "    - duraçãoo\n",
        "    - data e hora de transmissão\n",
        "    - canal de distribuição\n",
        "<br><br>\n",
        "\n",
        "Fluxo Oracle -> Apache Spark -> HBase executa todas as noites\n",
        "<br><br>\n",
        "\n",
        "- 10x ganho com 18 cores vs 1 core\n",
        "- com muitos dados, era inviável executar sem cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCLVcshx9Lfy"
      },
      "source": [
        "## 1.4 - Spark x Hadoop MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyUiT-5V9QdE"
      },
      "source": [
        "### O que é o Apache Hadoop?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xmH7S09qY-"
      },
      "source": [
        "- MapReduce nasceu em 2004, no Google\n",
        "- Hadoop nasceu em 2006\n",
        "- escrito em Java "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYJtAg_UJFoA"
      },
      "source": [
        "### Módulos do Hadoop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gYP8GiPJJz-"
      },
      "source": [
        "- Hadoop Common\n",
        "- Hadoop Distributed File System (HDFS)\n",
        "- Hadoop YARN\n",
        "- Hadoop Ozone\n",
        "- Hadoop MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9QlpSRBJw2E"
      },
      "source": [
        "### O Modelo de Programação MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzqSG2PJ09F"
      },
      "source": [
        "- *map:* filtra e ordena dados ao expô-los como pares chave-valor\n",
        "- *reduce:* a partir do mapeamento, sumariza os dados\n",
        "- inspirado em programação funcional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kw-8PQ_LC0B"
      },
      "source": [
        "```\n",
        "function map(String name, String document):\n",
        "// name: document name\n",
        "// document: document contents\n",
        "for each word w in document: \n",
        "    emit (w, 1)\n",
        "```\n",
        "\n",
        "```\n",
        "function reduce(String word, Iterator partialCounts):\n",
        "// word: a word\n",
        "// partialCounts: a list of aggregated partial counts\n",
        "sum = 0\n",
        "for each pc in partialCounts:\n",
        "    sum += pc\n",
        "emit (word, sum)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsRaJKx_MemR"
      },
      "source": [
        "### Desempenho: Spark vs Hadoop MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RPj-1u9M0Mt"
      },
      "source": [
        "- Spark pode ser entre 10 e 100 vezes mais veloz que o Hadoop MapReduce\n",
        "    - Spark faz menos leituras e escritas em disco\n",
        "    - Spark armazena dados intermediários em memória sempre que possível\n",
        "- Spark brilha em tarefas iterativas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g49Lw-JxNTTp"
      },
      "source": [
        "### Spark pode ser executado com o YARN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38QV8NT-NW-Q"
      },
      "source": [
        "- YARN = Yet Another Resource Negotiator\n",
        "    - gestor de recursos do cluster\n",
        "    - agendamento e monitoramento de jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqO-mSMFN3V_"
      },
      "source": [
        "### Spark pode ser executado com HDFS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKF4n1ioN6pA"
      },
      "source": [
        "- HDFS = Hadoop Distributed File System\n",
        "    -  armazena grandes conjuntos de dados em hardware commodity\n",
        "    - centenas / milhares de nós\n",
        "    - tolerância a falhas via réplicas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvN_uaUfOXAh"
      },
      "source": [
        "### Tolerância a falhas: Spark vs Hadoop MapReduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqjsXjHGOc0t"
      },
      "source": [
        "- ambos são tolerantes a falhas\n",
        "- como o Hadoop usa mais o disco, ele consegue se recuperar de falhas mais rapidamente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6IfvtmeOnKs"
      },
      "source": [
        "### Spark: plataforma unificada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzP8rbVCO5jT"
      },
      "source": [
        "- Hadoop: batch\n",
        "- Spark: batch, tempo real, iterativo, grafos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq4XcgvrPCUa"
      },
      "source": [
        "### Spark e Hadoop podem cooperar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgiEgi15PPHn"
      },
      "source": [
        "- Apache Spark como motor de processamento dos dados\n",
        "- Apache Hadoop como armazenamento distribuído e gestor de recursos\n",
        "    - HDFS\n",
        "    - YARN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsj_v2QnPmSu"
      },
      "source": [
        "# Capítulo 2: Conceitos Fundamentais do Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlDGUoXfQJHu"
      },
      "source": [
        "## 2.1 - A Arquitetura do Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOtyFpOfQMyH"
      },
      "source": [
        "### Componentes do Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyWtNb8Eu17V"
      },
      "source": [
        "- driver program\n",
        "- cluster manager\n",
        "- worker nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AJrfPllYMxf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}